{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"annotation_parser.py\n",
    "\n",
    "This module provides utilities to parse annotations from various formats (txt,\n",
    "line, and XML) and map them to their corresponding images. The primary use\n",
    "cases include:\n",
    "\n",
    "1. Parsing annotations from .txt files, especially those in YOLO format.\n",
    "2. Extracting bounding box annotations from dataset files where each line\n",
    "represents an image and its associated bounding boxes.\n",
    "3. Converting XML annotations, typically in PASCAL VOC format, into a\n",
    "structured format.\n",
    "\n",
    "Given a directory or file with annotations, the module offers functions to\n",
    "extract these annotations, adjust any necessary file extensions, and create a\n",
    "structured Pandas DataFrame. This DataFrame can then be exported to a JSON\n",
    "format for further processing, analysis, or model training.\n",
    "\n",
    "The module consolidates the functionality of previously separate utilities into\n",
    "a single, cohesive class, making it easier to handle various annotation formats\n",
    "in a unified manner.\n",
    "\n",
    "Returns:\n",
    "    DataFrame: A Pandas DataFrame containing the parsed annotations mapped to\n",
    "    their respective images.\n",
    "\n",
    "Usage:\n",
    "    To load the config file into the class:\n",
    "        parser = AnnotationParser()\n",
    "        parser.load_config('config.ini')\n",
    "        parser.process_all_datasets()\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import configparser\n",
    "import pandas as pd\n",
    "\n",
    "XFILE_DIR = './data/processed/xml/'\n",
    "TXT_DIR = './data/processed/txt/'\n",
    "OUTPUT = './data/processed/annotations.json'\n",
    "DS_TRAIN = './data/processed/trn'\n",
    "DS_TEST = './data/processed/test'\n",
    "\n",
    "\n",
    "class AnnotationParser:\n",
    "    \"\"\"\n",
    "    A utility class for parsing image annotations from various formats.\n",
    "\n",
    "    The AnnotationParser class provides methods to extract annotations from\n",
    "    different formats such as .txt (YOLO format), line-based datasets, and XML\n",
    "    (typically PASCAL VOC format). It consolidates the functionality of\n",
    "    previously separate utilities into a unified interface, allowing for\n",
    "    streamlined processing and conversion of annotations into a structured\n",
    "    Pandas DataFrame. This DataFrame can then be exported to a JSON format\n",
    "    for further processing or analysis.\n",
    "\n",
    "    Attributes:\n",
    "        OUTPUT (str): Default path for the output JSON file.\n",
    "        FOLDER_IN (str): Default directory containing images and their\n",
    "          annotations.\n",
    "        DS3_TRN, DS3_TST, DS4_TRN, DS4_TST (str): Default paths for dataset\n",
    "          files.\n",
    "        XFILE_DIR (str): Default directory for XML files.\n",
    "\n",
    "    Methods:\n",
    "        load_config(config_file): Load parameters from a configuration file.\n",
    "        process_all_datasets(): Process all dataset constants and create\n",
    "        annotation sets for each.\n",
    "        ... [other methods]\n",
    "\n",
    "    Usage:\n",
    "        parser = AnnotationParser()\n",
    "        parser.load_config('config.ini')\n",
    "        parser.process_all_datasets()\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A Pandas DataFrame containing the parsed annotations mapped\n",
    "        to their respective images.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_all_datasets(self):\n",
    "        \"\"\"\n",
    "        Process all dataset constants and create annotation sets for each.\n",
    "\n",
    "        This function iterates through each of the dataset constants, reads the\n",
    "        dataset, creates a DataFrame of annotations, and then exports the\n",
    "        DataFrame to a JSON file.\n",
    "        \"\"\"\n",
    "        datasets = [DS_TRAIN, DS_TEST]\n",
    "        for dataset in datasets:\n",
    "            df = self.line_to_dataframe(dataset)\n",
    "            json_filename = dataset.replace(\".txt\", \".json\").replace(\n",
    "                \"data/processed/\", \"data/processed/json/\"\n",
    "            )\n",
    "            df.to_json(json_filename, orient=\"records\", lines=True)\n",
    "            print(f\"Processed {dataset} and saved to {json_filename}\")\n",
    "\n",
    "    def line_to_dataframe(self, dataset):\n",
    "        \"\"\"\n",
    "        Read the dataset file and return a DataFrame with image paths and\n",
    "        annotations.\n",
    "\n",
    "        This function reads the specified dataset file, parses each line using\n",
    "        the parse_line function, and aggregates the parsed data into a\n",
    "        structured Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Path to the dataset file.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A Pandas DataFrame containing the image paths and their\n",
    "            associated bounding box annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file=dataset, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        data = [self.from_line(line) for line in lines]\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def xml_to_annotations(xml_string):\n",
    "        \"\"\"Convert a given XML string to the consistent annotation format.\"\"\"\n",
    "        root = ET.fromstring(xml_string)\n",
    "        image_path = root.find('path').text if root.find('path') is not None else root.find('filename').text\n",
    "        width = float(root.find('size/width').text)\n",
    "        height = float(root.find('size/height').text)\n",
    "        annotations = []\n",
    "        for obj in root.findall('object'):\n",
    "            annotation = {\n",
    "                'class': obj.find('name').text,\n",
    "                'xmin': float(obj.find('bndbox/xmin').text),\n",
    "                'ymin': float(obj.find('bndbox/ymin').text),\n",
    "                'xmax': float(obj.find('bndbox/xmax').text),\n",
    "                'ymax': float(obj.find('bndbox/ymax').text)\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'annotations': annotations\n",
    "        }\n",
    "\n",
    "    def parse_yolo_line(self, line, image_dir):\n",
    "        \"\"\"Parse a YOLO-formatted line and extract the annotation data.\"\"\"\n",
    "        parts = line.strip().split()\n",
    "        image_path = os.path.join(image_dir, parts[0])\n",
    "        width = 720\n",
    "        height = 720\n",
    "        annotations = [{\n",
    "            'class': 'pothole',\n",
    "            'xmin': int((float(parts[1]) - float(parts[3])/2) * width),\n",
    "            'ymin': int((float(parts[2]) - float(parts[4])/2) * height),\n",
    "            'xmax': int((float(parts[1]) + float(parts[3])/2) * width),\n",
    "            'ymax': int((float(parts[2]) + float(parts[4])/2) * height)\n",
    "        }]\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'annotations': annotations\n",
    "        }\n",
    "\n",
    "    def parse_dataset_line(self, line):\n",
    "        \"\"\"Parse a dataset line and extract the annotation data.\"\"\"\n",
    "        parts = line.strip().split()\n",
    "        image_path = parts[0]\n",
    "        num_boxes = int(parts[1])\n",
    "        annotations = []\n",
    "        for i in range(2, 2 + 4 * num_boxes, 4):\n",
    "            annotation = {\n",
    "                'class': 'pothole',  # Placeholder, as class is not provided in this format\n",
    "                'xmin': float(parts[i]),\n",
    "                'ymin': float(parts[i+1]),\n",
    "                'xmax': float(parts[i] + parts[i+2]),\n",
    "                'ymax': float(parts[i+1] + parts[i+3])\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'annotations': annotations\n",
    "        }\n",
    "\n",
    "    def process_xml_directory(self, xml_directory):\n",
    "        \"\"\"Process all XML files in a directory and return their annotations.\"\"\"\n",
    "        all_xml_annotations = []\n",
    "        for filename in os.listdir(xml_directory):\n",
    "            if filename.endswith('.xml'):\n",
    "                xml_file_path = os.path.join(xml_directory, filename)\n",
    "                with open(file=xml_file_path, mode='r', encoding='utf-8') as f:\n",
    "                    xml_data = f.read()\n",
    "                all_xml_annotations.append(self.xml_to_annotations(xml_data))\n",
    "        return all_xml_annotations\n",
    "\n",
    "    def process_yolo_directory(self, yolo_directory, image_dir):\n",
    "        \"\"\"Process all YOLO-formatted files in a directory and return their annotations.\"\"\"\n",
    "        all_yolo_annotations = []\n",
    "        for yolo_file in os.listdir(yolo_directory):\n",
    "            if yolo_file.endswith('.txt' or '.csv'):\n",
    "                yolo_file_path = os.path.join(yolo_directory, yolo_file)\n",
    "                with open(file=yolo_file_path, mode='r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    all_yolo_annotations.append(self.parse_yolo_line(line, image_dir))\n",
    "        return all_yolo_annotations\n",
    "\n",
    "    def process_all_files(self, xml_directory=XFILE_DIR, yolo_directory=TXT_DIR, image_dir=DS_TRAIN):\n",
    "        \"\"\"Process all XML and YOLO-formatted files and save the results to a JSON file.\"\"\"\n",
    "        all_xml_annotations = self.process_xml_directory(xml_directory)\n",
    "        all_yolo_annotations = self.process_yolo_directory(yolo_directory, image_dir)\n",
    "\n",
    "        # Convert to DataFrame and save to JSON\n",
    "        df_xml = pd.DataFrame(all_xml_annotations)\n",
    "        df_yolo = pd.DataFrame(all_yolo_annotations)\n",
    "\n",
    "        # Combine both DataFrames\n",
    "        combined_df = pd.concat([df_xml, df_yolo], ignore_index=True)\n",
    "\n",
    "        # Save to JSON\n",
    "        combined_df.to_json(OUTPUT, orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"preprocessor.py\n",
    "Annotation Preprocessing Utility.\n",
    "\n",
    "This module provides a utility class for preprocessing image annotations from\n",
    "various formats. It allows for loading annotations, splitting datasets based on\n",
    "provided criteria, and combining annotations from different datasets into a\n",
    "unified format.\n",
    "\n",
    "Returns:\n",
    "    DataFrame: A Pandas DataFrame containing the preprocessed annotations.\n",
    "\n",
    "prep2.py\n",
    "This module provides a DatasetPreprocessor class to preprocess various datasets\n",
    "for object detection tasks. The class includes methods to handle different\n",
    "formats of datasets and convert them into a standardized format suitable for\n",
    "object detection models. The standardized format includes center coordinates,\n",
    "width-height format for bounding boxes, normalized coordinates, and mapped\n",
    "class labels to class IDs.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    A utility class for preprocessing image annotations.\n",
    "\n",
    "    The Preprocessor class provides methods to load annotations from JSON\n",
    "    files, split datasets based on provided splits, and combine annotations\n",
    "    from different datasets into a unified format.\n",
    "\n",
    "    Attributes:\n",
    "        parser (AnnotationParser): An instance of the AnnotationParser class.\n",
    "        annotations (dict): A dictionary to store loaded annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parser):\n",
    "        \"\"\"\n",
    "        Initialize the Preprocessor with a given parser.\n",
    "\n",
    "        Args:\n",
    "            parser (AnnotationParser): An instance of the AnnotationParser\n",
    "            class.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "        self.annotations = {}\n",
    "\n",
    "    def load_annotations(self, *json_files):\n",
    "        \"\"\"\n",
    "        Load annotations from the provided JSON files into a dictionary.\n",
    "\n",
    "        Given a list of JSON file paths, this method reads each file and stores\n",
    "        the annotations in a dictionary with the file name as the key.\n",
    "\n",
    "        Args:\n",
    "            *json_files (str): Paths to the JSON files containing annotations.\n",
    "        \"\"\"\n",
    "        for file in json_files:\n",
    "            df = pd.read_json(file, lines=True)\n",
    "            self.annotations[file] = df\n",
    "\n",
    "    # def split_dataset(self, splits_file):\n",
    "    #     \"\"\"\n",
    "    #     Split the dataset based on the provided splits file.\n",
    "\n",
    "    #     Given a splits file, this method divides the dataset into training and\n",
    "    #     validation sets based on the specified split criteria.\n",
    "\n",
    "    #     Args:\n",
    "    #         splits_file (str): Path to the JSON file containing split criteria.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     splits = pd.read_json(splits_file)\n",
    "    #     train_files = splits[splits[\"split\"] == \"train\"][\"filename\"].tolist()\n",
    "    #     val_files = splits[splits[\"split\"] == \"val\"][\"filename\"].tolist()\n",
    "\n",
    "    #     train_df = self.annotations[\"df1_annotations.json\"][\n",
    "    #         self.annotations[\"df1_annotations.json\"][\"filename\"].isin(train_files)\n",
    "    #     ]\n",
    "    #     val_df = self.annotations[\"df1_annotations.json\"][\n",
    "    #         self.annotations[\"df1_annotations.json\"][\"filename\"].isin(val_files)\n",
    "    #     ]\n",
    "\n",
    "    #     self.annotations[\"df1_train\"] = train_df\n",
    "    #     self.annotations[\"df1_val\"] = val_df\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Combine annotations from different datasets into a unified format.\n",
    "\n",
    "        This method aggregates annotations from different datasets into a\n",
    "        single DataFrame, ensuring a consistent format for further processing\n",
    "        or analysis.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A Pandas DataFrame containing the combined annotations.\n",
    "        \"\"\"\n",
    "        all_data = pd.concat(\n",
    "            [\n",
    "                df\n",
    "                for key, df in self.annotations.items()\n",
    "                if key not in [\"df1_annotations.json\", \"df1_splits.json\"]\n",
    "            ]\n",
    "        )\n",
    "        return all_data\n",
    "\n",
    "    def load_data(self, json_file, img_dir):\n",
    "        \"\"\"\n",
    "        Load image and mask data from the provided JSON file and image\n",
    "        directory.\n",
    "\n",
    "        Args:\n",
    "            json_file (str): Path to the JSON file containing image annotations.\n",
    "            img_dir (str): Directory containing the images referenced in the\n",
    "            JSON file.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two numpy arrays:\n",
    "            - images (numpy.ndarray): An array of normalized images of shape\n",
    "                (num_images, 128, 128, 3).\n",
    "            - masks (numpy.ndarray): An array of masks corresponding to the\n",
    "                images, where each mask is of shape (128, 128)\n",
    "                and contains binary values (0 or 1) indicating the absence or\n",
    "                presence of an object.\n",
    "        \"\"\"\n",
    "        df = pd.concat(pd.read_json(json_file, lines=True))\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        def process_row(row):\n",
    "            img_path = os.path.join(img_dir, row[\"image_path\"])\n",
    "            img = Image.open(img_path).resize((128, 128))\n",
    "            img_array = np.array(img) / 255.0  # Normalize\n",
    "            images.append(img_array)\n",
    "\n",
    "            mask = np.zeros((img.height, img.width))\n",
    "            for box in row[\"boxes\"]:\n",
    "                x, y, i, j = box[\"x\"], box[\"y\"], box[\"width\"], box[\"height\"]\n",
    "                mask[y : y + j, x : x + i] = 1\n",
    "            masks.append(mask)\n",
    "\n",
    "        df.apply(process_row, axis=1)\n",
    "\n",
    "        return np.array(images), np.array(masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN For Identifying Potholes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure that all the required scripts, listed below, are in the same directory as the notebook or are accessible via the Python path.\n",
    "    - [ ] `annotation_parser.py`\n",
    "    - [ ] `features_old.py`\n",
    "    - [ ] `feature_extractor.py`\n",
    "    - [ ] `preprocessor.py`\n",
    "    - [ ] `train_model.py`\n",
    "    - [ ] `predict_model.py`\n",
    "    - [ ] `visualize.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ensure that all the required data files, listed below are in the specified directories or adjust the paths in the notebook accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run each cell in the notebook sequentially.\n",
    "   1. preprocess the data\n",
    "   2. extract features\n",
    "   3. design the model\n",
    "   4. build and train the model\n",
    "   5. validate the model\n",
    "   6. visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/processed/xml/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m parser \u001b[39m=\u001b[39m AnnotationParser()\n\u001b[1;32m----> 8\u001b[0m parser\u001b[39m.\u001b[39;49mprocess_all_files()\n",
      "Cell \u001b[1;32mIn[9], line 209\u001b[0m, in \u001b[0;36mAnnotationParser.process_all_files\u001b[1;34m(self, xml_directory, yolo_directory, image_dir)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_all_files\u001b[39m(\u001b[39mself\u001b[39m, xml_directory\u001b[39m=\u001b[39mXFILE_DIR, yolo_directory\u001b[39m=\u001b[39mTXT_DIR, image_dir\u001b[39m=\u001b[39mDS_TRAIN):\n\u001b[0;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Process all XML and YOLO-formatted files and save the results to a JSON file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     all_xml_annotations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_xml_directory(xml_directory)\n\u001b[0;32m    210\u001b[0m     all_yolo_annotations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_yolo_directory(yolo_directory, image_dir)\n\u001b[0;32m    212\u001b[0m     \u001b[39m# Convert to DataFrame and save to JSON\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 187\u001b[0m, in \u001b[0;36mAnnotationParser.process_xml_directory\u001b[1;34m(self, xml_directory)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Process all XML files in a directory and return their annotations.\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m all_xml_annotations \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 187\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(xml_directory):\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.xml\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    189\u001b[0m         xml_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(xml_directory, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './data/processed/xml/'"
     ]
    }
   ],
   "source": [
    "from features.feature_extractor import FeatureExtractor as FEx\n",
    "from data.preprocessor import Preprocessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "parser = AnnotationParser()\n",
    "parser.process_all_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Preprocessing**\n",
    "\n",
    "This section is dedicated to preparing the data for the model. This involves loading the data, possibly normalizing or augmenting it, and splitting it into training, validation, and test sets. The `AnnotationParser` and `Preprocessor` classes are utilized here to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m parser \u001b[39m=\u001b[39m AnnotationParser()\n\u001b[0;32m      5\u001b[0m preprocessor \u001b[39m=\u001b[39m Preprocessor(parser)\n\u001b[1;32m----> 6\u001b[0m preprocessor\u001b[39m.\u001b[39;49mload_annotations(ANON)\n\u001b[0;32m      8\u001b[0m all_data \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mpreprocess()\n\u001b[0;32m     10\u001b[0m \u001b[39m# Splitting into training and temporary set\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# which will be further split into validation and test sets\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\AI\\AI-ML-Project\\src\\data\\preprocessor.py:62\u001b[0m, in \u001b[0;36mPreprocessor.load_annotations\u001b[1;34m(self, *json_files)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mLoad annotations from the provided JSON files into a dictionary.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m    *json_files (str): Paths to the JSON files containing annotations.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m json_files:\n\u001b[1;32m---> 62\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(file, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannotations[file] \u001b[39m=\u001b[39m df\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:784\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n\u001b[0;32m    783\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 784\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:973\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    971\u001b[0m         data \u001b[39m=\u001b[39m ensure_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    972\u001b[0m         data_lines \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 973\u001b[0m         obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_parser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_combine_lines(data_lines))\n\u001b[0;32m    974\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_object_parser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1001\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    999\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1001\u001b[0m     obj \u001b[39m=\u001b[39m FrameParser(json, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mparse()\n\u001b[0;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1004\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1134\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse()\n\u001b[0;32m   1136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1320\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1316\u001b[0m orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1320\u001b[0m         loads(json, precise_float\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecise_float), dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m \u001b[39melif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1323\u001b[0m     decoded \u001b[39m=\u001b[39m {\n\u001b[0;32m   1324\u001b[0m         \u001b[39mstr\u001b[39m(k): v\n\u001b[0;32m   1325\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m loads(json, precise_float\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecise_float)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1326\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# # Usage\n",
    "ANON = \"annotations.json\"\n",
    "\n",
    "parser = AnnotationParser()\n",
    "preprocessor = Preprocessor(parser)\n",
    "preprocessor.load_annotations(ANON)\n",
    "\n",
    "all_data = preprocessor.preprocess()\n",
    "\n",
    "# Splitting into training and temporary set\n",
    "# which will be further split into validation and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    all_data.drop('class_id', axis=1),\n",
    "    all_data['class_id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Splitting the temporary set into validation and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Features**\n",
    "\n",
    "This section typically involves feature extraction or engineering.\n",
    "\n",
    "For CNNs, the raw pixel values of the images are used as features.\n",
    "\n",
    "However, if there are any additional features that need to be extracted or engineered,\n",
    "they would be handled in this section. The `FeatureExtractor` class is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern, hog\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract various features from an image.\n",
    "\n",
    "    This class provides methods to extract features like LBP, Canny edges,\n",
    "    color histogram, HOG, and basic image statistics from a given image. It\n",
    "    also provides a method to extract all features and concatenate them into a\n",
    "    single vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, image):\n",
    "        self.image = cv2.cvtColorTwoPlane(\n",
    "            image, cv2.COLOR_BGR2GRAY\n",
    "        )  # Convert to grayscale for some features\n",
    "\n",
    "\n",
    "    def lbp(self, P=8, R=1):\n",
    "        \"\"\"\n",
    "        Compute Local Binary Pattern (LBP) for the image.\n",
    "\n",
    "        LBP is a simple yet efficient texture operator which labels the pixels\n",
    "        of an image by thresholding the neighborhood of each pixel and\n",
    "        considers the result as a binary number.\n",
    "\n",
    "        Args:\n",
    "            P (int, optional): Number of circularly symmetric neighbor set\n",
    "                points. Defaults to 8.\n",
    "            R (int, optional): Radius of circle. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: LBP image.\n",
    "        \"\"\"\n",
    "        return local_binary_pattern(self.image, P=P, R=R, method=\"uniform\")\n",
    "\n",
    "\n",
    "    def canny_edge(self, lower_threshold=100, upper_threshold=200):\n",
    "        \"\"\"\n",
    "        Compute Canny edge detection for the image.\n",
    "\n",
    "        The Canny edge detection operator was developed by John F. Canny in\n",
    "        1986 and uses a multi-stage algorithm to detect a wide range of edges\n",
    "        in images.\n",
    "\n",
    "        Args:\n",
    "            lower_threshold (int, optional): First threshold for the hysteresis\n",
    "                procedure. Defaults to 100.\n",
    "            upper_threshold (int, optional): Second threshold for the\n",
    "                hysteresis procedure. Defaults to 200.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Binary image of edges.\n",
    "        \"\"\"\n",
    "        return cv2.Canny(self.image, lower_threshold, upper_threshold)\n",
    "\n",
    "\n",
    "    def color_histogram(self, bins=8):\n",
    "        \"\"\"\n",
    "        Compute color histogram for the grayscale image.\n",
    "\n",
    "        A histogram represents the distribution of pixel intensities in an\n",
    "        image.\n",
    "\n",
    "        Args:\n",
    "            bins (int, optional): Number of bins for the histogram. Default is 8\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Flattened histogram array.\n",
    "        \"\"\"\n",
    "        hist = cv2.calcHist([self.image], [0], None, [bins], [0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "\n",
    "\n",
    "    def hog_features(self, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "        \"\"\"\n",
    "        Compute Histogram of Oriented Gradients (HOG) for the image.\n",
    "\n",
    "        HOG is a feature descriptor used in object detection.\n",
    "\n",
    "        Args:\n",
    "            pixels_per_cell (tuple, optional): Size (in pixels) of a cell. Defaults to (8, 8).\n",
    "            cells_per_block (tuple, optional): Number of cells in each block. Defaults to (2, 2).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: HOG feature vector.\n",
    "        \"\"\"\n",
    "        return hog(\n",
    "            self.image,\n",
    "            pixels_per_cell=pixels_per_cell,\n",
    "            cells_per_block=cells_per_block,\n",
    "            visualize=False,\n",
    "        )\n",
    "\n",
    "\n",
    "    def image_statistics(self):\n",
    "        \"\"\"\n",
    "        Compute basic image statistics: mean, median, standard deviation.\n",
    "\n",
    "        These statistics provide basic information about the pixel intensity\n",
    "        distribution in the image.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Mean, median, and standard deviation of the image.\n",
    "        \"\"\"\n",
    "        mean = np.mean(self.image)\n",
    "        median = np.median(self.image)\n",
    "        std = np.std(self.image)\n",
    "        return mean, median, std\n",
    "\n",
    "\n",
    "    def extract_all(self):\n",
    "        \"\"\"\n",
    "        Extract all features and concatenate them into a single vector.\n",
    "\n",
    "        This method extracts LBP, Canny edges, color histogram, HOG, and basic\n",
    "        image statistics and concatenates them to form a single feature vector.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Concatenated feature vector.\n",
    "        \"\"\"\n",
    "        lbp_hist = np.histogram(self.lbp(), bins=8, range=(0, 256))[0]\n",
    "        canny_edges = self.canny_edge().flatten()\n",
    "        color_hist = self.color_histogram()\n",
    "        hog_feat = self.hog_features()\n",
    "        stats = self.image_statistics()\n",
    "\n",
    "        # Concatenate all features into a single vector\n",
    "        return np.concatenate([lbp_hist, canny_edges, color_hist, hog_feat, stats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "xtra = FEx.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Model Design**\n",
    "\n",
    "This is where the architecture of the CNN model is defined. The `design_model` function from the `train_model` script is used to create the model architecture.\n",
    "\n",
    "Here, we'll define the architecture of the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def design_model():\n",
    "    \"\"\"\n",
    "    Design a convolutional neural network (CNN) for binary image classification.\n",
    "\n",
    "    This function defines a CNN architecture using Keras Sequential API. The\n",
    "    model consists of convolutional layers, max-pooling layers, a flatten\n",
    "    layer, and dense layers. The final layer uses a sigmoid activation function\n",
    "    for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        keras.models.Model: A Keras model with the defined CNN architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation='relu',\n",
    "            input_shape=(128, 128, 3)\n",
    "        ),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def my_optimizer():\n",
    "    \"\"\"\n",
    "    Define the Adam optimizer for training the model.\n",
    "\n",
    "    This function specifies the Adam optimizer with a learning rate of 0.0001.\n",
    "\n",
    "    Returns:\n",
    "        keras.optimizers.Adam: Adam optimizer with the specified learning rate.\n",
    "    \"\"\"\n",
    "    return Adam(lr=0.0001)\n",
    "\n",
    "\n",
    "# Model Design\n",
    "model = design_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Model Build**\n",
    "\n",
    "After defining the model architecture, this section is dedicated to compiling the model, setting any callbacks, and training the model using the training data. The `build_model` function from the `train_model` script is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Build\n",
    "\n",
    "def build_model(model, X_train, y_train, X_val, y_val): # pylint: disable=C0103\n",
    "    \"\"\"\n",
    "    Compile and train the provided CNN model using the training and validation\n",
    "    data.\n",
    "\n",
    "    This function compiles the provided model using binary cross-entropy loss\n",
    "    and the Adam optimizer. It then trains the model using the provided\n",
    "    training data and validates it using the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Model): The CNN model to be trained.\n",
    "        X_train (numpy.ndarray): Training images.\n",
    "        y_train (numpy.ndarray): Training labels.\n",
    "        X_val (numpy.ndarray): Validation images.\n",
    "        y_val (numpy.ndarray): Validation labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A trained Keras model and its training history.\n",
    "    \"\"\"\n",
    "    model.compile(\n",
    "        optimizer=my_optimizer(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "model, history = build_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Validation**:\n",
    "\n",
    "Once the model is trained, it's important to evaluate its performance on a validation or test set to understand how well it's likely to perform on unseen data. The `validate_model` function from the `predict_model` script is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def validate_model(model, X_test, y_test): # pylint: disable=C0103\n",
    "    \"\"\"\n",
    "    Validate the performance of a trained model on test data.\n",
    "\n",
    "    This function evaluates the provided model using the test data and prints\n",
    "    the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Model): The trained CNN model.\n",
    "        X_test (numpy.ndarray): Test images.\n",
    "        y_test (numpy.ndarray): True labels for the test images.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the test data.\n",
    "    \"\"\"\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def make_predictions(model, X_new): # pylint: disable=C0103\n",
    "    \"\"\"\n",
    "    Make predictions on new, unseen data using the trained model.\n",
    "\n",
    "    This function uses the provided model to make predictions on a batch of new\n",
    "    images. It returns the predicted labels for each image.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Model): The trained CNN model.\n",
    "        X_new (numpy.ndarray): New images for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted labels for the new images.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_new)\n",
    "    predicted_labels = np.where(predictions > 0.5, 1, 0).flatten()\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting potholes using machine learning\n",
    "make_predictions()\n",
    "\n",
    "# Validation\n",
    "validate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Visualization**\n",
    "\n",
    "This section seems to be dedicated to visualizing the results and understanding the model's performance in more detail. The `NeuralNetworkVisualizer` class is used here for various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from visualize import NeuralNetworkVisualizer as nnv\n",
    "\n",
    "nnv.visualize_activation_maps(model, history)\n",
    "nnv.calculate_auc()\n",
    "nnv.plot_roc_curve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
