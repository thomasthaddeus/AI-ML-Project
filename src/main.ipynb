{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN For Identifying Potholes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure that all the required scripts, listed below, are in the same directory as the notebook or are accessible via the Python path.\n",
    "    - [ ] `annotation_parser.py`\n",
    "    - [ ] `features_old.py`\n",
    "    - [ ] `feature_extractor.py`\n",
    "    - [ ] `preprocessor.py`\n",
    "    - [ ] `train_model.py`\n",
    "    - [ ] `predict_model.py`\n",
    "    - [ ] `visualize.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ensure that all the required data files, listed below are in the specified directories or adjust the paths in the notebook accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run each cell in the notebook sequentially. The notebook will:\n",
    "   1. preprocess the data\n",
    "   2. extract features\n",
    "   3. design the model\n",
    "   4. build and train the model\n",
    "   5. validate the model\n",
    "   6. visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If there are any errors or issues\n",
    "   1. they will likely be raised when the corresponding cell is run.\n",
    "   2. Ensure that all dependencies are installed and that there are no issues with the data or scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Python\\AI\\AI-ML-Project\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.train_model import build_model, design_model\n",
    "from models.predict_model import validate_model, make_predictions\n",
    "from data.annotation_parser import AnnotationParser\n",
    "from features.feature_extractor import FeatureExtractor as FEx\n",
    "from data.preprocessor import Preprocessor, DatasetPreprocessor as dataprep\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "parser = AnnotationParser()\n",
    "parser.process_all_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Preprocessing**\n",
    "\n",
    "This section is dedicated to preparing the data for the model. This involves loading the data, possibly normalizing or augmenting it, and splitting it into training, validation, and test sets. The `AnnotationParser` and `Preprocessor` classes are utilized here to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 224\n",
    "\n",
    "# Load datasets\n",
    "df3_proc = dataprep.ds3_4(data3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "df4_proc = dataprep.ds3_4(data4, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "# Concatenate all datasets\n",
    "all_data = pd.concat([df1_proc, df2_proc, df3_proc, df4_proc], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage\n",
    "ANON = \"src/data/annotations.json\"\n",
    "\n",
    "parser = AnnotationParser()\n",
    "parser.load_config('config.ini')\n",
    "preprocessor = Preprocessor(parser)\n",
    "preprocessor.load_annotations(ANON)\n",
    "\n",
    "all_data = preprocessor.preprocess()\n",
    "\n",
    "# Splitting into training and temporary set\n",
    "# which will be further split into validation and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    all_data.drop('class_id', axis=1),\n",
    "    all_data['class_id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Splitting the temporary set into validation and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Features**\n",
    "\n",
    "This section typically involves feature extraction or engineering.\n",
    "\n",
    "For CNNs, the raw pixel values of the images are used as features.\n",
    "\n",
    "However, if there are any additional features that need to be extracted or engineered,\n",
    "they would be handled in this section. The `FeatureExtractor` class is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "xtra = FEx.extract_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Model Design**\n",
    "\n",
    "This is where the architecture of the CNN model is defined. The `design_model` function from the `train_model` script is used to create the model architecture.\n",
    "\n",
    "Here, we'll define the architecture of the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "model = design_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Model Build**\n",
    "\n",
    "After defining the model architecture, this section is dedicated to compiling the model, setting any callbacks, and training the model using the training data. The `build_model` function from the `train_model` script is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Build\n",
    "model, history = build_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Validation**:\n",
    "\n",
    "Once the model is trained, it's important to evaluate its performance on a validation or test set to understand how well it's likely to perform on unseen data. The `validate_model` function from the `predict_model` script is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting potholes using machine learning\n",
    "make_predictions()\n",
    "\n",
    "# Validation\n",
    "validate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Visualization**\n",
    "\n",
    "This section seems to be dedicated to visualizing the results and understanding the model's performance in more detail. The `NeuralNetworkVisualizer` class is used here for various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from visualize import NeuralNetworkVisualizer as nnv\n",
    "\n",
    "nnv.visualize_activation_maps(model, history)\n",
    "nnv.calculate_auc()\n",
    "nnv.plot_roc_curve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
